# Описание
Была реализована RAG система с историей сообщений, которая отвечает на вопросы по Terraria Wiki.
- Dataset: parkourer10/terraria-wiki
- Embeddings: paraphrase-multilingual-MiniLM-L12-v2

За векторню БД взята Chroma. Embeddings строятся по вопросам в dataset, а ответ на вопрос сохраняется в metadata. Поиск происходит внутри Langchain, в результате возвращается 5 ближайших документов. Перед поиском документов, llm на основе контекста и истории сообщений переформулирует запрос к Chroma (из трех тестируемых моделей лучше всего показал себя qwen3). После получения документов поле answer из metadata попадает в промпт для генерации ответа от модели.

P.S. В ветке master лежит реализация с Langfuse, где можно удобно посмотреть как модель переформулировала вопрос для поиска в БД и какие документы были по этому запросу получены. В данной ветке Langfuse убарл, потому что очень сильно загружает ПК.


# Стек
- Python 3.11
- Ollama
- Langchain
- Сhroma - векторная БД для хранения Dataset с Terraria Wiki

# Запуск
#### Вариант 1. Dev Container 
- Запустить Dev Container
- Скопировать .env.example в .env (переменные уже настроены)
- Установить pip install -r requirement.txt
- Выполнить команду task up для запуска сервиса
- Дождаться пока Ollama скачает модели (Модели указываются в env OLLAMA_MODELS и качаются автоматически, после запуска контейнера)
- Запустить скрипт src/main.py (Сама RAG)
- Запустить скрипт src/search.py (Запросы в Chroma и получение 3 документов)
- При желании, можно изменить список загружаемых моделей в env (необходим перезапуск контейнера Ollama)
- При желании, можно изменить используемую модель (В самом файле main.py из списка OLLAMA_MODELS)
- При желании, можно изменить список вопросов к модели (В файле main.py)

#### Вариант 2. Ручной запуск
- Скопировать .env.example в .env
- Изменить значение переменных в .env OLLAMA_HOST, CHROMA_URL и LANGFUSE_HOST на `localhost`
- Установить pip install -r requirement.txt
- Выполнить команду docker compose up -d для запуска сервиса
- Дождаться пока Ollama скачает модели (Модели указываются в env OLLAMA_MODELS и качаются автоматически, после запуска контейнера)
- Запустить скрипт src/main.py (Сама RAG)
- Запустить скрипт src/search.py (Запросы в Chroma и получение 3 документов)
- При желании, можно изменить список загружаемых моделей в env (необходим перезапуск контейнера Ollama)
- При желании, можно изменить используемую модель (В самом файле main.py из списка OLLAMA_MODELS)
- При желании, можно изменить список вопросов к модели (В файле main.py)